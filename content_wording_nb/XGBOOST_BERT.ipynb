{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ6pwNMP6F2c233AHpdfhs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanPasidu/ScriboSense/blob/saadha/XGBOOST_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOk_h_I_5g2X",
        "outputId": "15feb3d0-1358-4c2f-f610-0fb8b1428136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "UfdqL-O3OXxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_path = '/content/drive/My Drive/Student Summarizer/dataset/'\n",
        "\n",
        "# Read CSV file\n",
        "summaries = pd.read_csv(dataset_path + 'summaries_train.csv')\n",
        "prompts = pd.read_csv(dataset_path + 'prompts_train.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(summaries.head())\n",
        "print(prompts.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugForNaKp1JH",
        "outputId": "82e4665d-7467-4c38-c445-0905a7bd78e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     student_id prompt_id                                               text  \\\n",
            "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
            "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
            "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
            "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
            "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
            "\n",
            "    content   wording  \n",
            "0  0.205683  0.380538  \n",
            "1 -0.548304  0.506755  \n",
            "2  3.128928  4.231226  \n",
            "3 -0.210614 -0.471415  \n",
            "4  3.272894  3.219757  \n",
            "  prompt_id                                    prompt_question  \\\n",
            "0    39c16e  Summarize at least 3 elements of an ideal trag...   \n",
            "1    3b9047  In complete sentences, summarize the structure...   \n",
            "2    814d6b  Summarize how the Third Wave developed over su...   \n",
            "3    ebad26  Summarize the various ways the factory would u...   \n",
            "\n",
            "                prompt_title  \\\n",
            "0                 On Tragedy   \n",
            "1  Egyptian Social Structure   \n",
            "2             The Third Wave   \n",
            "3    Excerpt from The Jungle   \n",
            "\n",
            "                                         prompt_text  \n",
            "0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n",
            "1  Egyptian society was structured like a pyramid...  \n",
            "2  Background \\r\\nThe Third Wave experiment took ...  \n",
            "3  With one member trimming beef in a cannery, an...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LQs8Fz3tQya",
        "outputId": "f2913a8d-b01c-41aa-9ac9-77e70b7c7e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "_mname = 'bert-large-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(_mname)\n",
        "model = AutoModel.from_pretrained(_mname)\n"
      ],
      "metadata": {
        "id": "op5c45wytN3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "ygyrRvBYwAHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = summaries.merge(prompts, on='prompt_id')"
      ],
      "metadata": {
        "id": "HQf9_xcgwB2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9apUp0kwO9G",
        "outputId": "fbf220ca-3163-4a30-a6b4-865f5157e791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 7165 entries, 0 to 7164\n",
            "Data columns (total 8 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   student_id       7165 non-null   object \n",
            " 1   prompt_id        7165 non-null   object \n",
            " 2   text             7165 non-null   object \n",
            " 3   content          7165 non-null   float64\n",
            " 4   wording          7165 non-null   float64\n",
            " 5   prompt_question  7165 non-null   object \n",
            " 6   prompt_title     7165 non-null   object \n",
            " 7   prompt_text      7165 non-null   object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 503.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    df['text'] = df['text'].str.lower()\n",
        "    df['prompt_text'] = df['prompt_text'].str.lower()\n",
        "    df['prompt_question'] = df['prompt_question'].str.lower()\n",
        "    return df\n",
        "\n",
        "df = preprocess(df)"
      ],
      "metadata": {
        "id": "iDEqpqKOwY74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['student_id', 'prompt_id', 'prompt_title']\n",
        "\n",
        "df = df.drop(columns=columns_to_drop)\n"
      ],
      "metadata": {
        "id": "5vribaFg0-cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sKV7AiH1xOYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "LKPn4XlYxXdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooling_method = 'mean'\n",
        "\n",
        "def pool(output, method):\n",
        "    if method == 'cls':\n",
        "        return output.last_hidden_state[:, 0, :]\n",
        "    elif method == 'pooler':\n",
        "        return output.pooler_output\n",
        "    elif method == 'mean':\n",
        "        return output.last_hidden_state.mean(dim=1)\n",
        "    elif method == 'max':\n",
        "        return output.last_hidden_state.max(dim=1)[0]"
      ],
      "metadata": {
        "id": "nYHH1E6Gxr6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_vectorizer(texts):\n",
        "    batch_size = 84\n",
        "\n",
        "    # tokenize texts\n",
        "    encoded_input = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(encoded_input['input_ids']), batch_size)):\n",
        "            batch = {key: val[i:i+batch_size].to(device) for key, val in encoded_input.items()}\n",
        "            output = model(**batch)\n",
        "            if i == 0:\n",
        "                embeddings = pool(output, pooling_method).cpu().detach().numpy()\n",
        "            else:\n",
        "                embeddings = np.concatenate((embeddings, pool(output, pooling_method).cpu().detach().numpy()), axis=0)\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "0vn77np4yC-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_encode = ['text', 'prompt_text']\n",
        "\n",
        "X_train = []\n",
        "X_valid = []\n",
        "for feature in features_to_encode:\n",
        "    X_train.append(bert_vectorizer(df_train[feature].values.tolist()))\n",
        "    X_valid.append(bert_vectorizer(df_test[feature].values.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ562s5Jyetj",
        "outputId": "6e793083-4ee3-4932-d058-a91ad38f9292"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 29/69 [5:00:09<6:50:42, 616.07s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECK IF X[0] AND X[1] ARE COORECT , SINCE SOME COLUMNS HAVE BEEN DROPPED"
      ],
      "metadata": {
        "id": "L6_rtucjC6d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_features(X):\n",
        "    X_sub = X[0] - X[1]\n",
        "    X_mul = X[0] * X[1]\n",
        "    X.extend([X_sub, X_mul])\n",
        "    return X\n",
        "\n",
        "extended_features = True\n",
        "if extended_features and len(features_to_encode) > 1:\n",
        "    X_train = extend_features(X_train)\n",
        "    X_valid = extend_features(X_valid)"
      ],
      "metadata": {
        "id": "sBVzG_D2zEHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge features\n",
        "X_train = np.concatenate(X_train, axis=1)\n",
        "X_valid = np.concatenate(X_valid, axis=1)\n",
        "\n",
        "# Checking feature dimensions\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)"
      ],
      "metadata": {
        "id": "BcHrAtsIzRqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'objective': 'reg:squarederror',  # Regression task\n",
        "    'eval_metric': 'rmse',            # Root Mean Squared Error\n",
        "    'learning_rate': 0.1,             # Typical range: [0.01, 0.3]\n",
        "    'max_depth': 5,                   # Typical range: [3, 10]\n",
        "    'min_child_weight': 0,            # Controls overfitting, set as 1 for small datasets\n",
        "    'gamma': 0,                       # Regularization parameter, typical range: [0, 5]\n",
        "    'subsample': 0.8,                 # Fraction of samples used for training, typical range: [0.5, 1]\n",
        "    'colsample_bytree': 0.8,          # Fraction of features used for training, typical range: [0.5, 1]\n",
        "    'n_estimators': 300,              # Number of boosting rounds\n",
        "    'early_stopping_rounds': 10,      # Stop training if performance doesn't improve for N rounds\n",
        "    'random_state': 42                # Seed for reproducibility\n",
        "}\n",
        "content_model = XGBRegressor(**params)\n",
        "content_model.fit(X_train, df_train['content'], eval_set=[(X_valid, df_test['content'])])\n",
        "\n",
        "wording_model = XGBRegressor(**params)\n",
        "wording_model.fit(X_train, df_train['wording'], eval_set=[(X_valid, df_test['wording'])])\n",
        "\n",
        "# Predict\n",
        "y_pred_c = content_model.predict(X_valid)\n",
        "y_pred_w = wording_model.predict(X_valid)\n",
        "\n",
        "# Evaluate (MSE) and MCRMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def mcrmse(y_trues, y_preds):\n",
        "    # calc individual rmse\n",
        "    rmses = []\n",
        "    for i in range(len(y_trues)):\n",
        "        mse = mean_squared_error(y_trues[i], y_preds[i])\n",
        "        rmse = np.sqrt(mse)\n",
        "        rmses.append(rmse)\n",
        "\n",
        "    # calc mean of rmse\n",
        "    return np.mean(rmses)\n",
        "\n",
        "\n",
        "mse_c = mean_squared_error(df_test['content'], y_pred_c)\n",
        "mse_w = mean_squared_error(df_test['wording'], y_pred_w)\n",
        "print(f'MSE content: {mse_c:.4f}')\n",
        "print(f'MSE wording: {mse_w:.4f}')\n",
        "mcrmse = mcrmse([df_test['content'], df_test['wording']], [y_pred_c, y_pred_w])\n",
        "print(f'MCRMSE: {mcrmse:.4f}')"
      ],
      "metadata": {
        "id": "G84rBg57zVS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "df_test_prompts = pd.read_csv(os.path.join(DATA_DIR, 'prompts_test.csv'))\n",
        "df_test_summaries = pd.read_csv(os.path.join(DATA_DIR, 'summaries_test.csv'))\n",
        "\n",
        "df_test = df_test_summaries.merge(df_test_prompts, on='prompt_id')\n",
        "\n",
        "# Common preprocessing steps\n",
        "df_test = preprocess(df_test)\n",
        "\n",
        "X_test = []\n",
        "for feature in features_to_encode:\n",
        "    X_test.append(bert_vectorizer(df_test[feature].values.tolist()))\n",
        "\n",
        "if extended_features and len(features_to_encode) > 1:\n",
        "    X_test = extend_features(X_test)\n",
        "\n",
        "X_test = np.concatenate(X_test, axis=1)\n",
        "# X_test = bert_vectorizer(df_test['text'].values.tolist())\n",
        "\n",
        "y_pred_c = content_model.predict(X_test)\n",
        "y_pred_w = wording_model.predict(X_test)\n",
        "\n",
        "# Save predictions\n",
        "df_test['content'] = y_pred_c\n",
        "df_test['wording'] = y_pred_w\n",
        "df_test[['student_id', 'content', 'wording']].to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "nn1qaRjwzgp7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
