{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1467849a-6144-4598-8080-2aa752f5bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "import string\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from rouge import Rouge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fcf493c-61e5-4b4a-917e-e48adb40fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/model/LR_model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd14caf3-c5a8-4821-a641-f6e43948efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3627dab3-840e-483f-8d7f-2741d6e969e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(match):\n",
    "    num = match.group()\n",
    "    num_word = inflect_instance.number_to_words(num)\n",
    "    return num_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd839336-5e7d-40ce-bd99-c0902ebacc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_features(generated_summary, reference_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "    return scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04f4e4fa-1b89-4b55-b152-1dcf41731526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level_metrics(summary):\n",
    "    # Compute sentence-level metrics\n",
    "    sentences = summary.split('. ')  # Split into sentences\n",
    "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "    readability_scores = [TextBlob(sentence).sentiment.polarity for sentence in sentences]\n",
    "    return sentence_lengths, readability_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7305afe-c94f-418d-b637-d175959f1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings(summary):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the summary text\n",
    "    tokens = tokenizer.tokenize(summary)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Generate BERT-based embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        word_embeddings = outputs.last_hidden_state.squeeze().numpy()\n",
    "\n",
    "    return word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03fb8fea-7149-48b5-8969-21a4df22da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_features(generated_summary, reference_summary):\n",
    "    # Compute TF-IDF cosine similarity\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([generated_summary, reference_summary])\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4bd477f-8aa1-48f2-b053-47c6155de8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflect_instance = inflect.engine()\n",
    "\n",
    "# Regular expression pattern to find numbers in the text\n",
    "pattern = r'\\d+(\\.\\d+)?'\n",
    "ps = PorterStemmer()\n",
    "# nltk.download('punkt',download_dir='../static/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "680c289e-e466-49a2-bbdd-a980cd2f5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocessing(input):\n",
    "    \n",
    "    cols = input.columns\n",
    "    for col in cols:\n",
    "        input[col] = input[col].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "        input[col] = input[col].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in x.split()))\n",
    "        input[col] = input[col].apply(remove_punctuations)\n",
    "        input[col] = input[col].apply(lambda x: re.sub(pattern, replace_numbers, x))\n",
    "        input[col] = input[col].apply(lambda x: \" \".join(ps.stem(x) for x in x.split()))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8e6d1db-fe33-4d95-a37c-45c19288435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "def generate_summary(input):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = \"google/pegasus-large\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    generated_summaries = []\n",
    "    \n",
    "    # Loop through each row in the dataset\n",
    "    for index, row in input.iterrows():\n",
    "        prompt_text = row['prompt_txt']\n",
    "        prompt_question = row['prompt_question']\n",
    "        prompt_title = row['prompt_title']\n",
    "        \n",
    "        # Combine prompts\n",
    "        input_text = f\"{prompt_text} {prompt_question} {prompt_title}\"\n",
    "        \n",
    "        # Tokenize input and generate summary\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(input_ids)\n",
    "        generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Append the generated summary to the list\n",
    "        generated_summaries.append(generated_summary)\n",
    "    \n",
    "    # Add the generated summaries to the dataset as a new column\n",
    "    input['generated_summary'] = generated_summaries\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50e00490-f79a-46a5-8b0f-a4fa65c872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(input):\n",
    "    # Calculate features for each summary\n",
    "\n",
    "    # Initialize lists to store feature values\n",
    "    rouge_1_score = []\n",
    "    rouge_2_score = []\n",
    "    sentence_lengths = []\n",
    "    readability_scores = []\n",
    "    word_embeddings = []\n",
    "    content_sim =[]\n",
    "    cosine_similarities = []\n",
    "    \n",
    "    # Iterate through each row in the feature_dataset\n",
    "    for index, row in input.iterrows():\n",
    "        reference_summary = row['summary']\n",
    "        generated_summary = row['generated_summary']\n",
    "        \n",
    "        # Calculate features using your existing functions\n",
    "        rouge_score_1,rouge_score_2 = rouge_features(reference_summary, generated_summary)\n",
    "        sentence_length, readability_score = sentence_level_metrics(reference_summary)\n",
    "        embedding = text_embeddings(reference_summary)\n",
    "        content_based_feature = content_based_features(reference_summary, generated_summary)\n",
    "        \n",
    "        # Calculate cosine similarity between embeddings\n",
    "        cosine_sim = cosine_similarity([embedding.mean(axis=0)], [embedding.mean(axis=0)])[0][0]\n",
    "        \n",
    "        # Append feature values to lists\n",
    "        rouge_1_score.append(rouge_score_1)\n",
    "        rouge_2_score.append(rouge_score_2)\n",
    "        sentence_lengths.append(sentence_length[0])\n",
    "        readability_scores.append(readability_score[0])\n",
    "        word_embeddings.append(embedding)\n",
    "        content_sim.append(content_based_feature[0][0])\n",
    "        cosine_similarities.append(cosine_sim)\n",
    "    \n",
    "    # Create a new DataFrame with calculated features\n",
    "    feature_columns = ['rouge_1_score','rouge_2_score', 'sentence_length', 'readability_score', 'word_embedding','content_sim', 'cosine_similarity']\n",
    "    features_df = pd.DataFrame(zip(rouge_1_score,rouge_2_score, sentence_lengths, readability_scores, word_embeddings, content_sim,cosine_similarities), columns=feature_columns)\n",
    "    \n",
    "    # Concatenate the original dataset and the calculated features\n",
    "    final_dataset = pd.concat([input, features_df], axis=1)\n",
    "    print(final_dataset)\n",
    "    return final_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65749335-e885-4cb8-b3b4-26e21b098702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_score(input):\n",
    "    content = model.predict(input)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cc97782-2d3d-49a6-b927-b1ad317a6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = \"aaaaa\"\n",
    "prompt_title = 'bbbbb'\n",
    "prompt_txt = \"ccccc\"\n",
    "summary = \"dddd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb863858-f3f3-43b8-8537-013bf5239723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame\n",
    "data = {'prompt_question': [prompt_question], 'prompt_title': [prompt_title], 'prompt_txt': [prompt_txt], 'summary': [summary]}\n",
    "input = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33d264f7-7178-4809-a502-47d645784d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prompt_question prompt_title prompt_txt summary  generated_summary  \\\n",
      "0           aaaaa        bbbbb      ccccc    dddd  ccccc aaaaa bbbbb   \n",
      "\n",
      "   rouge_1_score  rouge_2_score  sentence_length  readability_score  \\\n",
      "0            0.0            0.0                1                0.0   \n",
      "\n",
      "                                      word_embedding  content_sim  \\\n",
      "0  [[0.32850066, 0.1855365, -0.14470264, -0.97448...          0.0   \n",
      "\n",
      "   cosine_similarity  \n",
      "0                1.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.6247952])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_input = preprocessing(input)\n",
    "input_with_sumary = generate_summary(preprocessed_input)\n",
    "feature_vals = calculate_features(input_with_sumary)\n",
    "content = get_content_score(feature_vals.drop(columns=['prompt_question','prompt_title','prompt_txt','summary','generated_summary','word_embedding']))\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5b1cc97-0c65-4f1c-9bc9-58c23955a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e767efe-90b6-4337-8e48-cad0ec16acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
