{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1467849a-6144-4598-8080-2aa752f5bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "import string\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from rouge import Rouge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6fcf493c-61e5-4b4a-917e-e48adb40fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/model/LR_model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fd14caf3-c5a8-4821-a641-f6e43948efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3627dab3-840e-483f-8d7f-2741d6e969e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(match):\n",
    "    num = match.group()\n",
    "    num_word = inflect_instance.number_to_words(num)\n",
    "    return num_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bd839336-5e7d-40ce-bd99-c0902ebacc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_features(generated_summary, reference_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "    return scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "04f4e4fa-1b89-4b55-b152-1dcf41731526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level_metrics(summary):\n",
    "    # Compute sentence-level metrics\n",
    "    sentences = summary.split('. ')  # Split into sentences\n",
    "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "    readability_scores = [TextBlob(sentence).sentiment.polarity for sentence in sentences]\n",
    "    return sentence_lengths, readability_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c7305afe-c94f-418d-b637-d175959f1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings(summary):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the summary text\n",
    "    tokens = tokenizer.tokenize(summary)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Generate BERT-based embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        word_embeddings = outputs.last_hidden_state.squeeze().numpy()\n",
    "\n",
    "    return word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "03fb8fea-7149-48b5-8969-21a4df22da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_features(generated_summary, reference_summary):\n",
    "    # Compute TF-IDF cosine similarity\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([generated_summary, reference_summary])\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c4bd477f-8aa1-48f2-b053-47c6155de8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflect_instance = inflect.engine()\n",
    "\n",
    "# Regular expression pattern to find numbers in the text\n",
    "pattern = r'\\d+(\\.\\d+)?'\n",
    "ps = PorterStemmer()\n",
    "# nltk.download('punkt',download_dir='../static/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "680c289e-e466-49a2-bbdd-a980cd2f5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocessing(input):\n",
    "    \n",
    "    cols = input.columns\n",
    "    for col in cols:\n",
    "        input[col] = input[col].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "        input[col] = input[col].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in x.split()))\n",
    "        input[col] = input[col].apply(remove_punctuations)\n",
    "        input[col] = input[col].apply(lambda x: re.sub(pattern, replace_numbers, x))\n",
    "        input[col] = input[col].apply(lambda x: \" \".join(ps.stem(x) for x in x.split()))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e8e6d1db-fe33-4d95-a37c-45c19288435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "def generate_summary(input):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = \"google/pegasus-large\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    generated_summaries = []\n",
    "    \n",
    "    # Loop through each row in the dataset\n",
    "    for index, row in input.iterrows():\n",
    "        prompt_text = row['prompt_txt']\n",
    "        prompt_question = row['prompt_question']\n",
    "        prompt_title = row['prompt_title']\n",
    "        \n",
    "        # Combine prompts\n",
    "        input_text = f\"{prompt_text} {prompt_question} {prompt_title}\"\n",
    "        \n",
    "        # Tokenize input and generate summary\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(input_ids)\n",
    "        generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Append the generated summary to the list\n",
    "        generated_summaries.append(generated_summary)\n",
    "    \n",
    "    # Add the generated summaries to the dataset as a new column\n",
    "    input['generated_summary'] = generated_summaries\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "50e00490-f79a-46a5-8b0f-a4fa65c872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(input):\n",
    "    # Calculate features for each summary\n",
    "\n",
    "    # Initialize lists to store feature values\n",
    "    rouge_1_score = []\n",
    "    rouge_2_score = []\n",
    "    sentence_lengths = []\n",
    "    readability_scores = []\n",
    "    word_embeddings = []\n",
    "    content_sim =[]\n",
    "    cosine_similarities = []\n",
    "    \n",
    "    # Iterate through each row in the feature_dataset\n",
    "    for index, row in input.iterrows():\n",
    "        reference_summary = row['summary']\n",
    "        generated_summary = row['generated_summary']\n",
    "        \n",
    "        # Calculate features using your existing functions\n",
    "        rouge_score_1,rouge_score_2 = rouge_features(reference_summary, generated_summary)\n",
    "        sentence_length, readability_score = sentence_level_metrics(reference_summary)\n",
    "        embedding = text_embeddings(reference_summary)\n",
    "        content_based_feature = content_based_features(reference_summary, generated_summary)\n",
    "        \n",
    "        # Calculate cosine similarity between embeddings\n",
    "        cosine_sim = cosine_similarity([embedding.mean(axis=0)], [embedding.mean(axis=0)])[0][0]\n",
    "        \n",
    "        # Append feature values to lists\n",
    "        rouge_1_score.append(rouge_score_1)\n",
    "        rouge_2_score.append(rouge_score_2)\n",
    "        sentence_lengths.append(sentence_length[0])\n",
    "        readability_scores.append(readability_score[0])\n",
    "        word_embeddings.append(embedding)\n",
    "        content_sim.append(content_based_feature[0][0])\n",
    "        cosine_similarities.append(cosine_sim)\n",
    "    \n",
    "    # Create a new DataFrame with calculated features\n",
    "    feature_columns = ['rouge_1_score','rouge_2_score', 'sentence_length', 'readability_score', 'word_embedding','content_sim', 'cosine_similarity']\n",
    "    features_df = pd.DataFrame(zip(rouge_1_score,rouge_2_score, sentence_lengths, readability_scores, word_embeddings, content_sim,cosine_similarities), columns=feature_columns)\n",
    "    \n",
    "    # Concatenate the original dataset and the calculated features\n",
    "    final_dataset = pd.concat([input, features_df], axis=1)\n",
    "    return final_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "65749335-e885-4cb8-b3b4-26e21b098702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_score(input):\n",
    "    content = model.predict(input)\n",
    "    return content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9cc97782-2d3d-49a6-b927-b1ad317a6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = \"summarize at least three elements of an ideal tragedy as described by aristotle\"\n",
    "prompt_title = 'on tragedy'\n",
    "prompt_txt = \"chapter thirteen as the sequel to what has already been said we must proceed to consider what the poet should aim at and what he should avoid in constructing his plots and by what means the specific effect of tragedy will be produced a perfect tragedy should as we have seen be arranged not on the simple but on the complex plan it should moreover imitate actions which excite pity and fear this being the distinctive mark of tragic imitation it follows plainly in the first place that the change of fortune presented must not be the spectacle of a virtuous man brought from prosperity to adversity for this moves neither pity nor fear it merely shocks us nor again that of a bad man passing from adversity to prosperity for nothing can be more alien to the spirit of tragedy it possesses no single tragic quality it neither satisfies the moral sense nor calls forth pity or fear nor again should the downfall of the utter villain be exhibited a plot of this kind would doubtless satisfy the moral sense but it would inspire neither pity nor fear for pity is aroused by unmerited misfortune fear by the misfortune of a man like ourselves such an event therefore will be neither pitiful nor terrible there remains then the character between these two extremes — that of a man who is not eminently good and just yet whose misfortune is brought about not by vice or depravity but by some error of judgement or frailty he must be one who is highly renowned and prosperous — a personage like oedipus thyestes or other illustrious men of such families a wellconstructed plot should therefore be single in its issue rather than double as some maintain the change of fortune should be not from bad to good but reversely from good to bad it should come about as the result not of vice but of some great error or frailty in a character either such as we have described or better rather than worse the practice of the stage bears out our view at first the poets recounted any legend that came in their way now the best tragedies are founded on the story of a few houses — on the fortunes of alcmaeon oedipus orestes meleager thyestes telephus and those others who have done or suffered something terrible a tragedy then to be perfect according to the rules of art should be of this construction hence they are in error who censure euripides just because he follows this principle in his plays many of which end unhappily it is as we have said the right ending the best proof is that on the stage and in dramatic competition such plays if well worked out are the most tragic in effect and euripides faulty though he may be in the general management of his subject yet is felt to be the most tragic of the poets in the second rank comes the kind of tragedy which some place first like the odyssey it has a double thread of plot and also an opposite catastrophe for the good and for the bad it is accounted the best because of the weakness of the spectators for the poet is guided in what he writes by the wishes of his audience the pleasure however thence derived is not the true tragic pleasure it is proper rather to comedy where those who in the piece are the deadliest enemies — like orestes and aegisthus — quit the stage as friends at the close and no one slays or is slain\"\n",
    "summary = \"1 element of an ideal tragedy is that it should be arranged on a complex plan.  Another element of an ideal tragedy is that it should only have one main issue. The last element of an ideal tragedy is that it should have a double thread plot and an opposite catastrophe for both good and bad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cb863858-f3f3-43b8-8537-013bf5239723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame\n",
    "data = {'prompt_question': [prompt_question], 'prompt_title': [prompt_title], 'prompt_text': [prompt_txt], 'summary': [summary]}\n",
    "input = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "33d264f7-7178-4809-a502-47d645784d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_input = preprocessing(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a6013478-ea92-4ea0-8d5b-febd9382f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summ = pd.read_csv('../artifacts/generated_summary.csv')\n",
    "\n",
    "if prompt_title == 'on tragedy' and prompt_question== \"summarize at least three elements of an ideal tragedy as described by aristotle\":\n",
    "    item_to_extract = generated_summ.at[0, 'generated_summary']\n",
    "    preprocessed_input['generated_summary'] = item_to_extract\n",
    "    input_with_sumary = preprocessed_input\n",
    "elif prompt_title == 'egyptian social structure' and prompt_question== \"in complete sentences summarize the structure of the ancient egyptian system of government how were different social classes involved in this government cite evidence from the text\":\n",
    "    item_to_extract = generated_summ.at[1, 'generated_summary']\n",
    "    preprocessed_input['generated_summary'] = item_to_extract\n",
    "    input_with_sumary = preprocessed_input\n",
    "elif prompt_title == 'the third wave' and prompt_question== \"summarize how the third wave developed over such a short period of time and why the experiment was ended\":\n",
    "    item_to_extract = generated_summ.at[2, 'generated_summary']\n",
    "    preprocessed_input['generated_summary'] = item_to_extract\n",
    "    input_with_sumary = preprocessed_input\n",
    "elif prompt_title == 'excerpt from the jungle' and prompt_question== \"summarize the various ways the factory would use or cover up spoiled meat cite evidence in your answer\":\n",
    "    item_to_extract = generated_summ.at[3, 'generated_summary']\n",
    "    preprocessed_input['generated_summary'] = item_to_extract\n",
    "    input_with_sumary = preprocessed_input\n",
    "else:\n",
    "    input_with_sumary = generate_summary(preprocessed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e1b00124-14fb-4b58-9274-1eb5f3abefc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vals = calculate_features(input_with_sumary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e908a49-3323-4080-b336-8c2f088aff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_content_score(feature_vals.drop(columns=['prompt_question','prompt_title','prompt_text','summary','generated_summary','word_embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b5b1cc97-0c65-4f1c-9bc9-58c23955a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
